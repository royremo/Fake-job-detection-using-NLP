{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7441544,"sourceType":"datasetVersion","datasetId":4331287}],"dockerImageVersionId":30635,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/royremo/fake-job-detection-for-new-data?scriptVersionId=160483667\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# Create an empty a df_new_data DataFrame for storing the user input history\n\n#df_new_data = pd.DataFrame(columns=['role', 'location', 'job_description'])","metadata":{"execution":{"iopub.status.busy":"2024-01-20T14:21:27.992259Z","iopub.execute_input":"2024-01-20T14:21:27.993504Z","iopub.status.idle":"2024-01-20T14:21:27.999149Z","shell.execute_reply.started":"2024-01-20T14:21:27.993461Z","shell.execute_reply":"2024-01-20T14:21:27.997732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Download the lexicon\nimport nltk\nfrom nltk.corpus import opinion_lexicon\n\n# Get positive and negative words from the lexicon\npositive_words = set(opinion_lexicon.positive())\nnegative_words = set(opinion_lexicon.negative())","metadata":{"execution":{"iopub.status.busy":"2024-01-20T14:28:03.481239Z","iopub.execute_input":"2024-01-20T14:28:03.48168Z","iopub.status.idle":"2024-01-20T14:28:03.512942Z","shell.execute_reply.started":"2024-01-20T14:28:03.481643Z","shell.execute_reply":"2024-01-20T14:28:03.512065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle\nimport pandas as pd\nimport numpy as np\nimport nltk\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom concurrent.futures import ProcessPoolExecutor\nfrom nltk.tokenize import word_tokenize\nfrom sklearn.metrics import precision_recall_curve\nimport matplotlib.pyplot as plt\n\n# Welcome message\nprint(\"Welcome aboard! Let's navigate through job postings and identify the authentic ones.\\nInput the job information to begin!\\n\")\n\n\n# Load the Model back from file\nPkl_Filename = \"/kaggle/input/pkl-file-model-with-preprocesssing/Fake_Job_Postings_Detection_with_preprocessing.pkl\"\nwith open(Pkl_Filename, 'rb') as file:  \n    loaded_model_data = pickle.load(file)\n\n# Extract the loaded data\nloaded_ensemble_model = loaded_model_data['model']\ntfidf_vectorizer = loaded_model_data['tfidf_vectorizer']\nscaler = loaded_model_data['scaler']\nmin_max_scaler = loaded_model_data['min_max_scaler']\nnumeric_columns = loaded_model_data['numeric_columns']\noptimal_threshold_new_data = loaded_model_data['optimal_threshold_new_data']\n\n# Create or load df_new_data\ntry:\n    df_new_data = pd.read_csv(\"df_new_data.csv\")\nexcept FileNotFoundError:\n    df_new_data = pd.DataFrame(columns=['role', 'location', 'job_description', 'combined_text', 'fraudulent', 'positive_score', 'negative_score'])\n\n# Assuming df_new_data has columns: 'role', 'location', 'job_description'\n# Get user input for new entries\nuser_role = input(\"Enter the role: \")\nuser_location = input(\"Enter the location: \")\nuser_job_description = input(\"Enter the job description: \")\n\n# Create a new entry DataFrame with the user input\nnew_entry = pd.DataFrame({\n    'role': [user_role],\n    'location': [user_location],\n    'job_description': [user_job_description]\n})\n\n# If needed, fill any missing values in the new entry\nnew_entry = new_entry.fillna('')\n\n# Combine 'role', 'location', and 'job_description' into a single column\nnew_entry['combined_text'] = new_entry['role'] + ' ' + new_entry['location'] + ' ' + new_entry['job_description']\n\n# Calculate sentiment scores using the provided function\ndef calculate_sentiment_scores(row):\n    tokens = word_tokenize(row['combined_text'])\n    positive_score = np.sum(np.isin(tokens, list(positive_words)))\n    negative_score = np.sum(np.isin(tokens, list(negative_words)))\n    return pd.Series({'positive_score': positive_score, 'negative_score': negative_score})\n\n# Apply sentiment analysis to the new entry\nnew_entry[['positive_score', 'negative_score']] = new_entry.apply(calculate_sentiment_scores, axis=1)\n\n# Create additional features: 'desc_num_char', 'desc_num_words', 'desc_num_sent'\nnew_entry['desc_num_char'] = new_entry['combined_text'].apply(len)\nnew_entry['desc_num_words'] = new_entry['combined_text'].apply(lambda x: len(x.split()))\nnew_entry['desc_num_sent'] = new_entry['combined_text'].apply(lambda x: len(nltk.sent_tokenize(x)))\n\n# Create a DataFrame with numeric features\nX_numeric_new_entry = new_entry[['positive_score', 'negative_score', 'desc_num_char', 'desc_num_words', 'desc_num_sent']]\n\n# Combine text TF-IDF vectors with numeric features\nX_text_new_entry_tfidf = tfidf_vectorizer.transform(new_entry['combined_text'])\nX_new_entry = pd.concat([X_numeric_new_entry, pd.DataFrame(X_text_new_entry_tfidf.toarray())], axis=1)\n\n# Standardize numeric features\nX_new_entry.iloc[:, :5] = scaler.transform(X_new_entry.iloc[:, :5])\n\n# Apply Min-Max scaling to numeric features\nX_new_entry[numeric_columns] = min_max_scaler.transform(X_new_entry[numeric_columns])\n\n# Convert feature names to strings\nX_new_entry.columns = X_new_entry.columns.astype(str)\n\n# Predict probabilities for the new entry using the loaded ensemble model\ny_pred_prob_new_entry = loaded_ensemble_model.predict_proba(X_new_entry)[:, 1]\n\n# Apply optimal threshold\ny_pred_new_entry = (y_pred_prob_new_entry > optimal_threshold_new_data).astype(int)\n\n# Calculate sentiment scores for the new entry\nnew_entry[['positive_score', 'negative_score']] = new_entry.apply(calculate_sentiment_scores, axis=1)\n\n# Append the new entry to the existing df_new_data\nnew_entry['fraudulent'] = y_pred_new_entry\ndf_new_data = pd.concat([df_new_data, new_entry], ignore_index=True)\n\n# Save df_new_data to a CSV file\ndf_new_data.to_csv(\"df_new_data.csv\", index=False)\n\n# Display or use the predictions for the new entry\nprint(\"\\nPredictions for the New Entry:\")\nprint(y_pred_new_entry)\n# Print statements based on prediction\nif y_pred_new_entry[0] == 1:\n    print(\"Entered Job posting is Fake\")\nelse:\n    print(\"Entered Job posting is Real\")","metadata":{"execution":{"iopub.status.busy":"2024-01-20T14:34:48.06147Z","iopub.execute_input":"2024-01-20T14:34:48.061888Z","iopub.status.idle":"2024-01-20T14:35:12.03756Z","shell.execute_reply.started":"2024-01-20T14:34:48.061847Z","shell.execute_reply":"2024-01-20T14:35:12.036378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_new_data","metadata":{"execution":{"iopub.status.busy":"2024-01-20T14:35:17.470781Z","iopub.execute_input":"2024-01-20T14:35:17.471425Z","iopub.status.idle":"2024-01-20T14:35:17.487892Z","shell.execute_reply.started":"2024-01-20T14:35:17.471389Z","shell.execute_reply":"2024-01-20T14:35:17.486389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"        ","metadata":{},"execution_count":null,"outputs":[]}]}